# üìù MILESTONE 4: Level 2 Features - Enhancement (SELECTIVE)

**SEE ALSO**:
- [08-Process-Step-3-Overview.md](08-Process-Step-3-Overview.md) for principles
- [08A-Milestone-3-TAF-Core.md](08A-Milestone-3-TAF-Core.md) for M3 foundation

---

**Duration**: Varies (0 hours to 8 hours based on template and selected features)

**üéØ M·ª§C TI√äU M4**: M·ªü r·ªông kh·∫£ nƒÉng v√† t·ªëi ∆∞u hi·ªáu su·∫•t c·ªßa framework v·ªõi **Phase 2 & 3 features**

**‚ö†Ô∏è QUAN TR·ªåNG**: M4 CH·ªà implement c√°c features ƒë√£ ƒë∆∞·ª£c ch·ªçn trong Feature Selection Matrix (File 05)

---

## M4 - Decision Point

**Tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu M4, AI PH·∫¢I:**

```
üîç M4 DECISION CHECKLIST:

1. Ki·ªÉm tra Template ƒë√£ ch·ªçn (A/B/C/D/E)

2. Review Feature Selection Matrix cho template ƒë√≥:
   - Data-Driven Testing: Required/Optional/Skip?
   - ExtentReports: Required/Optional/Skip?
   - Retry Mechanism: Required/Optional/Skip?
   - Parallel Execution: Required/Optional/Skip?

3. X√°c nh·∫≠n v·ªõi user:
   "D·ª±a tr√™n Template [X], c√°c features recommend cho M4:

   ‚úÖ REQUIRED (Must implement):
   - [List]

   ‚≠ï OPTIONAL (Nice to have):
   - [List]

   ‚ùå SKIP (Not needed):
   - [List]

   B·∫°n c√≥ mu·ªën adjust list n√†y kh√¥ng?"

4. N·∫øu ALL features = SKIP ‚Üí SKIP M4 ho√†n to√†n, go to M5
```

---

## M4 - Implementation Strategy

**Process**: Implement **t·ª´ng feature m·ªôt**, test feature ƒë√≥, r·ªìi move next

**Th·ª© t·ª± ∆∞u ti√™n implement**:
1. ExtentReports (n·∫øu Required/Optional)
2. Data-Driven Testing (n·∫øu Required/Optional)
3. Retry Mechanism (n·∫øu Required/Optional)
4. Parallel Execution (n·∫øu Required/Optional)

---

## M4 - Feature 1: ExtentReports (Phase 2)

**When to implement**: Templates B, C, D, E (Required ho·∫∑c Optional)

**Duration**: 2 hours

**Files to create**:
- `utils/ExtentManager.java`
- Update `listeners/TestListener.java`

### Step 1: ExtentManager.java

```java
package utils;
import com.aventstack.extentreports.*;
import com.aventstack.extentreports.reporter.*;
import com.aventstack.extentreports.reporter.configuration.Theme;

public class ExtentManager {
    private static ExtentReports extent;
    private static ThreadLocal<ExtentTest> test = new ThreadLocal<>();

    public static ExtentReports getInstance() {
        if (extent == null) {
            ExtentSparkReporter sparkReporter = new ExtentSparkReporter("reports/ExtentReport.html");
            sparkReporter.config().setTheme(Theme.DARK);
            sparkReporter.config().setDocumentTitle("Test Automation Report");
            sparkReporter.config().setReportName("Functional Test Results");

            extent = new ExtentReports();
            extent.attachReporter(sparkReporter);
            extent.setSystemInfo("Environment", "QA");
            extent.setSystemInfo("Browser", "Chrome");
        }
        return extent;
    }

    public static ExtentTest getTest() { return test.get(); }
    public static void setTest(ExtentTest extentTest) { test.set(extentTest); }
    public static void flush() { if (extent != null) extent.flush(); }
}
```

### Step 2: Update TestListener.java

```java
// Add to TestListener.java

@Override
public void onStart(ITestContext context) {
    ExtentManager.getInstance();
    logger.info("Test suite started: {}", context.getName());
}

@Override
public void onTestStart(ITestResult result) {
    ExtentTest test = ExtentManager.getInstance()
        .createTest(result.getMethod().getMethodName());
    ExtentManager.setTest(test);
    logger.info("Test started: {}", result.getName());
}

@Override
public void onTestSuccess(ITestResult result) {
    ExtentManager.getTest().pass("Test passed");
    logger.info("Test passed: {}", result.getName());
}

@Override
public void onTestFailure(ITestResult result) {
    ExtentManager.getTest().fail(result.getThrowable());

    Object testClass = result.getInstance();
    WebDriver driver = ((tests.BaseTest) testClass).getDriver();

    if (driver != null) {
        String screenshotPath = ScreenshotHelper.capture(driver, result.getName());
        try {
            ExtentManager.getTest().addScreenCaptureFromPath(screenshotPath);
        } catch (Exception e) {
            logger.error("Failed to attach screenshot to report", e);
        }
    }
    logger.error("Test failed: {}", result.getName());
}

@Override
public void onFinish(ITestContext context) {
    ExtentManager.flush();
    logger.info("Test suite finished: {}", context.getName());
}
```

### Step 3: Test & Demo
- Run tests: `mvn test`
- Verify report generated: `reports/ExtentReport.html`
- Show user the beautiful HTML report
- Get approval

---

## M4 - Feature 2: Data-Driven Testing (Phase 2)

**When to implement**: Templates B, C, D, E

**Duration**: 2-3 hours

**Files to create**:
- `utils/DataHelper.java`
- `testdata/[entity]_testdata.json` (or .csv, .xlsx)

### Step 1: DataHelper.java

```java
package utils;
import com.fasterxml.jackson.databind.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import java.io.File;
import java.io.IOException;
import java.util.*;

public class DataHelper {
    private static final Logger logger = LoggerFactory.getLogger(DataHelper.class);

    public static Object[][] readJSON(String fileName) {
        try {
            ObjectMapper mapper = new ObjectMapper();
            JsonNode rootNode = mapper.readTree(new File("src/test/resources/testdata/" + fileName));

            List<Object[]> dataList = new ArrayList<>();
            for (JsonNode node : rootNode) {
                Object[] row = parseJsonNode(node);
                dataList.add(row);
            }
            return dataList.toArray(new Object[0][]);
        } catch (IOException e) {
            logger.error("Failed to read JSON file: {}", fileName, e);
            return new Object[0][0];
        }
    }

    private static Object[] parseJsonNode(JsonNode node) {
        // Implement based on your test data structure
        // Example: return new Object[]{node.get("field1").asText(), node.get("field2").asText()};
        return new Object[]{};
    }
}
```

### Step 2: Create test data file

```json
// testdata/users_testdata.json
[
  {
    "name": "John Doe",
    "email": "john@example.com",
    "age": 25
  },
  {
    "name": "Jane Smith",
    "email": "jane@example.com",
    "age": 30
  }
]
```

### Step 3: Use in Test

```java
@DataProvider(name = "userData")
public Object[][] getUserData() {
    return DataHelper.readJSON("users_testdata.json");
}

@Test(dataProvider = "userData")
public void testValidSubmission(String name, String email, int age) {
    page.fillName(name);
    page.fillEmail(email);
    page.fillAge(String.valueOf(age));
    page.clickSubmit();
    Assert.assertTrue(page.getSuccessMessage().contains("successful"));
}
```

### Step 4: Test & Demo
- Run tests with multiple data sets
- Show test runs multiple times with different data
- Get approval

---

## M4 - Feature 3: Retry Mechanism (Phase 3)

**When to implement**: Templates B, D (Required), C, E (Optional)

**Duration**: 1 hour

**Files to create**: `listeners/RetryAnalyzer.java`

### Implementation

```java
package listeners;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.testng.IRetryAnalyzer;
import org.testng.ITestResult;

public class RetryAnalyzer implements IRetryAnalyzer {
    private static final Logger logger = LoggerFactory.getLogger(RetryAnalyzer.class);
    private int retryCount = 0;
    private static final int MAX_RETRY = 2;

    @Override
    public boolean retry(ITestResult result) {
        if (retryCount < MAX_RETRY) {
            retryCount++;
            logger.warn("Retrying test '{}' - Attempt {}/{}",
                result.getName(), retryCount, MAX_RETRY);
            return true;
        }
        return false;
    }
}
```

### Usage in Test

```java
@Test(retryAnalyzer = RetryAnalyzer.class)
public void testFlakyScenario() {
    // Test will auto retry up to 2 times if fails
}
```

---

## M4 - Feature 4: Parallel Execution (Phase 3)

**When to implement**: Template B (Required), E (Optional)

**Duration**: 1-2 hours

**Files to create**: `testng-parallel.xml`

### Implementation

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE suite SYSTEM "https://testng.org/testng-1.0.dtd">
<suite name="Parallel Test Suite" parallel="methods" thread-count="3">
    <test name="Parallel Tests">
        <classes>
            <class name="tests.[TEST_CLASS]Test"/>
        </classes>
    </test>
</suite>
```

**Important**: Ensure WebDriver instances are thread-safe (use ThreadLocal in BaseTest)

---

## M4 - CHECKPOINT (Per Feature)

**Sau m·ªói feature implementation:**

```
‚úÖ FEATURE [X] HO√ÄN TH√ÄNH: [Feature Name]

Deliverable:
‚úì [Files created]

üì∏ DEMO:
[Show feature working]

üß™ TESTING:
[Instructions to verify]

---

‚ùì CHECKPOINT:

1. Feature c√≥ ho·∫°t ƒë·ªông ƒë√∫ng kh√¥ng?
2. C√≥ breaking changes kh√¥ng?
3. Approve ƒë·ªÉ ti·∫øp t·ª•c feature ti·∫øp theo?

‚úÖ Reply "Approved" ƒë·ªÉ continue
üîÑ Request changes n·∫øu c·∫ßn
```

**Loop cho t·ª´ng feature trong M4 list**

---

## M4 - FINAL CHECKPOINT

**Sau khi T·∫§T C·∫¢ M4 features ho√†n th√†nh:**

```
‚úÖ MILESTONE 4 HO√ÄN TH√ÄNH: Level 2 Features

Features implemented:
‚úì [List of features ƒë√£ implement]

Total files added: [X] files

---

‚ùì FINAL M4 CHECKPOINT:

1. T·∫•t c·∫£ features c√≥ ho·∫°t ƒë·ªông t·ªët c√πng nhau kh√¥ng?
2. Tests v·∫´n pass v·ªõi new features?
3. Performance c√≥ acceptable kh√¥ng?
4. Ready to move to M5 (Documentation)?

---

‚ö†Ô∏è **STOP COMMAND (CHO AI CHATBOT)**:

**D·ª™NG L·∫†I NGAY T·∫†I ƒê√ÇY**.

Kh√¥ng ƒë∆∞·ª£c vi·∫øt b·∫•t k·ª≥ d√≤ng code n√†o cho Milestone 5 (M5) cho ƒë·∫øn khi nh·∫≠n ƒë∆∞·ª£c x√°c nh·∫≠n **"APPROVED"** ch√≠nh th·ª©c t·ª´ ng∆∞·ªùi d√πng.

---

‚úÖ Reply "Approved" ƒë·ªÉ move to M5
üîÑ Request fixes n·∫øu c·∫ßn
```

---

**‚è≠Ô∏è NEXT**: After approval, proceed to [08C-Milestone-5-Documentation.md](08C-Milestone-5-Documentation.md)
